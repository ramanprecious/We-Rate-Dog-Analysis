{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting: wrangle_report\n",
    "* Create a **300-600 word written report** called \"wrangle_report.pdf\" or \"wrangle_report.html\" that briefly describes your wrangling efforts. This is to be framed as an internal document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data wrangling** is a variety of iterative processes designed to transform raw data into a readily usable format. It is the complete manipulation of data to enable the production of correct and meaningful results. It is an essential step in data analytics that could produce irreversible issues if not properly implemented.\n",
    "\n",
    "**WeRateDogs** is an international recognised account on the twitter platform, with over four million active followers, which engages its audience to rate posted dog images over 10. The dataset explored in this project was derived from three seperate pieces of data gathered from WeRateDogs using three data gathering techniques. These methods are **downloading locally**, **downloading programmatically** and **use of Twitter API**. \n",
    "\n",
    "Wrangling the WeRateDogs data involves the employment of three iterative processes which are: Gathering, Accessing and Cleaning. These wrangling stages are most important in the data analytics, data science, data engineering, business analytics, and artificial intelligence  world.\n",
    "\n",
    "**A. GATHERING:**\n",
    "\n",
    "Gathering the WeRateDogs dataset occured in three ways and they are: \n",
    "\n",
    "i. Directly downloading the twitter achive dataset locally and reading it into a pandas dataframe using the method 'read_csv()'.\n",
    "\n",
    "ii. Downloading the image prediction dataset programmatically over a url using the request library.\n",
    "\n",
    "iii. Additional data collection by querying the twitter API for each tweet's JSON data using Python's Tweepy library. This data is then  read to a dataframe line by line.\n",
    "\n",
    "**B. ACCESSING:**\n",
    "\n",
    "Accessing the collected datasets visually and programmatically, and several categories of quality and tidiness issues were identified. The issues are: \n",
    "\n",
    "_1. Quality-related issues_\n",
    "\n",
    "i. df_twitterA: Inconsistent data on the rating_denominator column.\n",
    "\n",
    "ii. df_twitterA: Missing, inconsistent, and wrong records on the name column.\n",
    "\n",
    "iii. df_twitterA: Incomplete records on the in_reply_to_status_id, in_reply_to_user_id, retweeted_status_id, retweeted_status_user_id, retweeted_status_timestamp, and expanded_urls  columns.\n",
    "\n",
    "iv. df_twitterA: Null values in columns doggo, pupper, puppo, and floofer represented with 'None' rather than 'NaN'.\n",
    "\n",
    "v. df_twitterA: Some records in the expanded_url column has more than one url represented.\n",
    "\n",
    "vi. df_twitterA: Many useful information combined within the text column.\n",
    "\n",
    "vii. df_imageP: p1, p2, and p3 columns have the combination of upper and lower case letters.\n",
    "\n",
    "viii. df_imageP: Column names are not very explanatory.\n",
    "\n",
    "ix. df_twitterA: Timestamp column is object instead of datatime datatype.\n",
    "\n",
    "x. df_twitterA: Unusually large ratings in rating_numerator column.\n",
    "\n",
    "xi. df_twitterA: Remove all retweets.\n",
    "\n",
    "xii. df_imageP: Many non-useful columns present\n",
    "\n",
    "xiii. df_imageP: Some images are not dogs according to the cnn algorithms.\n",
    "\n",
    "xiv. All: tweet_id column is integer instead of object datatype.\n",
    "\n",
    "_2. Tidiness issues_\n",
    "\n",
    "i. df_twitterA: Represent the dog stages columns (doggo, pupper, puppo, and floofer) properly.\n",
    "\n",
    "ii. All three dataset all belong to the same observational unit.\n",
    "\n",
    "**C. CLEANING:**\n",
    "\n",
    "Cleaning the dataset involved making copies of the original dataset to avoid loss of data and the employment of methods like .loc(), .drop(), .str.extract(), .to_datetime(), .str.split(), .notnull(), .str.lower(), .rename(), .astype(), .replace() etc. to thoroughly clean and ensure there is no underlining issue that could affect the result of the end visualization. Cleaning was done progammatically and ended with the merging of the three datasets (on the tweet_id primary key) and dropping of all non-useful columns. The cleaned dataset was finally saved as a csv file and read into a dataframe for other explorations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
